{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5728f78a",
   "metadata": {},
   "source": [
    "_Fictional Context For My Data Science Project as a Springboard Data Science Fellow for my Capstone 2 Project as of August 8th, 2022 (2022-08-08)_\n",
    "\n",
    "## Student Placement For An Institution of Higher Education\n",
    "\n",
    " <img src=\"https://github.com/BEGuzelsu/DataScience/blob/main/MBA_Placement_Project/Notebooks/Career-Fair.jpg?raw=true\" width=\"460\" height=\"345\"> \n",
    "\n",
    "### 1) Problem Formulation\n",
    "An organization for higher education has requested that data scientist takes a look at a sample of their student information to for two main purposes:\n",
    "* A model that can predict whether a student will find employment or not based on features provided the organization\n",
    "* Some preliminary work on model that can predict salary to see if a model can be considered for further contract work\n",
    "\n",
    "The goal is to provide students more information regarding their career trajectory so they can make more informed choices. While the data set we can used is locked (as the organization could only get an initial sign off from so many students to use their information for the model building stage), we are free to explore any model we wish in terms of developing the model.\n",
    "\n",
    "### 2) Data Source\n",
    "The data source for our project can be found [here](https://www.kaggle.com/datasets/benroshan/factors-affecting-campus-placement) as provided by the project sponser. We are fortunate that the data is relatively clean but does require some data wrangling to get into a state that we can begin to use for our data science project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd5fc98",
   "metadata": {},
   "source": [
    "### 3) Data Wrangling\n",
    "_For the full workthrough, please check the following notebook [here](https://github.com/BEGuzelsu/DataScience/blob/d6d4c4ba79aa461f962d358a000f9c739378d0f4/MBA_Placement_Project/Notebooks/2_Data-Wrangling_Capstone-2.ipynb)_\n",
    "\n",
    "We were given the following 14 features for 215 observations to develop our models on:\n",
    "* sl_no: Is the serial number for an observation (student going forward), which is effectively an index key\n",
    "* gender: Provides the gender for a student (M for Male, and F for Female)\n",
    "* ssc_p: The percentile Rank of the student in 10th grade\n",
    "* ssc_b: The Board of Education associated with the student's 10th grade class (Central or Other)\n",
    "* hsc_p: The percentile rank of the student in 12th grade\n",
    "* hsc_b: The Board of Education associated with the student's 12 grade class Central or Other)\n",
    "* hsc_s: The specialization of the student in 12th grade (Commerce, Science, or Other)\n",
    "* degree_p: The percentile rank of the student at completion of their undergradute degree\n",
    "* degree_t: The undergraduate degree that the student acquired which are Communications & Management (Comm&Mgmt), Science & Technology (Sci&Tech), or Other\n",
    "* workex: Whether the student has work experience or not\n",
    "* etest_p: The percentile rank of thes student on their employability test\n",
    "* specialization: The specialization of the student which are Marketing & Finance (Mkt&Fin) or Marketing & Human Resources (Mkt&HR)\n",
    "* mba_p: The percentile ranking of the student once they've completed their MBA\n",
    "* status: Whether a student has been placed (aka outcome) which are Placed or Not Placed\n",
    "* salary: The salary of a student upon placement\n",
    "\n",
    "\"Status\" will be the target feature for our first model to predict whether a student is employed or not. \"Salary\" will be the target feature for our second model to predict earnings.\n",
    "\n",
    "The main data \"wrangling\" and \"cleaning\" procedures we needed to utilize are listed below:\n",
    "1. Some categorical features that had only two possible outcomes (such as gender)  needed to be converted to a binary variable that could be used by various models. For example, we converted the \"gender\" feature to a \"female\" feature which is \"0\" if the student is male, and \"1\" if the student is female. This procedure was repeated for other features including (but not limited to) \"ssc_b\", \"hsc_b\", \"status\", and \"workex.\" In addition, feature names were adjusted for readability (e.g., \"workex\" was converted to \"work_experience\"\n",
    "2. For features that were categorical features with more than two possible outcomes (such as \"hsc_c\"), we create binary features to capture the different outcomes while leaving one category out to act as the base category. In the case of \"hsc_c\" which had three possible outcomes (i.e., \"Commerce\", \"Arts\", and \"Science\"), we leave the \"Arts\" category out as that becomes the base category and have binary features for \"Commerce\" and \"Science\"\n",
    "3. For the \"salary\" feature, if the student was unemployed, the salary was listed as \"NaN\" which correlated to a lack of employment. For the purposes of our modeling, the \"NaN\" outcome was replaced with zeroes.\n",
    "4. We checked any features that had an associated range (e.g., \"hsc_p\") to see if there were any outliers we'd need to consider. Outliers were determined using the IQR test of being either less than the 25% percentile value minus 1.5x the IQR or greater than the 75% percentile value plus 1.5 the IQR\n",
    "\n",
    "Once we completed all our procedures, we wrapped up data wrangling and moved to Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2cc6ab3",
   "metadata": {},
   "source": [
    "### 4) Exploratory Data Analysis\n",
    "\n",
    "_For the full-workthrough, please check the following notebook [here](https://github.com/BEGuzelsu/DataScience/blob/d6d4c4ba79aa461f962d358a000f9c739378d0f4/MBA_Placement_Project/Notebooks/3_EDA_Capstone-2.ipynb)_\n",
    "\n",
    "Once our data was wrangled and put into a proper form, we begin exploring our data to get a better understanding of our data and considerations for modeling.\n",
    "* One of the first things we noticed was that certain binary features (e.g., \"female\", \"hsc_Central\", etc.) didn't have a balanced 50/50 split between observations. While it's not necessarily important, it is something to keep in mind to check as once we begin splitting the sample for training and testing data sets, we may get some splits that are very sparsely populated in terms of representation of certain types of students.\n",
    "\n",
    "* In addition, any feature with an associated range, we checked via histogram to get an overall sense of the distribution. Nearly all our features (with the exception of \"salary\") had a relatively normal distribution shape with \"etest_p\" being the only other variable that appeared to be more uniform distribution.\n",
    "\n",
    "* Salary appeared to be heavily skewed to the right (excluding the zero observation) regardless of whether we included the most extreme outliers our not as represented below. This may require us to consider the distribution of salary when we are developing models (as well as the scale of the values)\n",
    "\n",
    "![alt text](Salary_hist_COMPLETE.png)\n",
    "\n",
    "* Now that we have a better understanding of all our features on their own, we begin looking at our features in relation to our target features as potential predictors. An easy place to begin here is with correlation heatmaps get a better understanding of magnitude and direction of relationships (below).\n",
    "\n",
    "![alt text](Corr_heatmap_COMPLETE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ba911b",
   "metadata": {},
   "source": [
    "#### _Exploring correlations with \"employed\"_\n",
    "\n",
    "* We first explored features that may be correlated with \"employed\" as the first model we need to build is associated with predicting whether a student will be employed. In exploring the data, we find that both the \"degree_p\" feature and the \"hsc_c\" feature appear to be correlated with the employed feature. As such we would expect these feature to help predict employability\n",
    "\n",
    "![alt text](hsc_degree_employed_scatter.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071e63a9",
   "metadata": {},
   "source": [
    "* In addition, we ran some t-tests to check if other categorical features had some predictive power. For example comparing students with a finance specialisation versus students with a HR specialisation gave us a p-value of <0.001 in terms of whether a student was employed or not. This would imply that a student's specialization does have a statistically significant effect on predicting employment status."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b94a2",
   "metadata": {},
   "source": [
    "#### _Exploring correlations with \"salary\"_\n",
    "\n",
    "* Since salary is a continuous feature, we want to check how some categorical features relate. An easy way to visualize this is with a swarmplot for some features like \"female\" and \"specialisation_finance.\" One of the main trends we can see is that there are some outliers that drag the swarms upward for one category or another and that there is a massive congregation of observatiosn in both categories around the 200k to 400k band of salaries\n",
    "\n",
    "![alt text](Swarm_COMPLETE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2fb3ce",
   "metadata": {},
   "source": [
    "* In addition we can create some straightforward scatterplots of continuous features against \"salary.\" One example would be for the feature \"etest_p\" which we fine \n",
    "\n",
    "![alt text](etest_salary_scatter_COMPLETE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bf0cee",
   "metadata": {},
   "source": [
    "* We find some correlation for feature like etest_p, but not the strongest correlation.\n",
    "\n",
    "Now that we've had a chance to explore our data and get a better sense of its structure and the relationships our features have with each other. The next step is to prepare our data for model usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e251de",
   "metadata": {},
   "source": [
    "### 5) Preprocessing\n",
    "\n",
    "_For the full-workthrough, please check the following notebook [here](https://github.com/BEGuzelsu/DataScience/blob/d6d4c4ba79aa461f962d358a000f9c739378d0f4/MBA_Placement_Project/Notebooks/4_PreProcessing_Capstone-2.ipynb)_\n",
    "\n",
    "In the preprocessing step our two main objectives are to:\n",
    "1. Remove any features that are not helpful in making predictions\n",
    "2. Transform our data to be more appropriate for model usage.\n",
    "\n",
    "In this step, features like \"sl_no\" were removed because the in essence act as an additional index that provide no real information. In addition, for categorical features with n different categories, at least one category had to be remove (e.g., Commerce, Science, and Arts) to make sure there were no co-linearity issues (i.e., drop Arts). One of the nice things of categorical binary features (0 or 1) is that they are naturally scaled in a nice manner.\n",
    "\n",
    "For the other continuous input features in our dataset, a Min Max Scaler approach was utilized to convert them to a range between 0 and 1 which puts them in line with our binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b79bf18",
   "metadata": {},
   "source": [
    "### 6) Modeling\n",
    "\n",
    "_For the full-workthrough, please check the following notebook [here](https://github.com/BEGuzelsu/DataScience/blob/d6d4c4ba79aa461f962d358a000f9c739378d0f4/MBA_Placement_Project/Notebooks/5_Modeling_Captone-2.ipynb)_\n",
    "\n",
    "Now that our data is fully prepared for modeling, we can begin developing our prediction models. First, we'll work on predicting employment\n",
    "\n",
    "#### _Modeling Employment_\n",
    "\n",
    "_Logistic Regression_\n",
    "\n",
    "Since we are working with a binary classification problem with predicting modeling, a good place to start is logistic regression. However, to do this properly, we also need to split our data set into a training set and a testing set. For our modeling, create a testing set that is 25% of the original data set. In addition, using Grid Search, we determine that the \"C\" parameter for our model should be \"1.\" The end result is an accuracy measure of 88.89%\n",
    "\n",
    "_Random Forest Classifier_\n",
    "\n",
    "Another good model to use for a problem like this is a Random Forest classifier. Using the same training/testing split and Grid Search, we find the best \"max depth\" to be 10 and the optimal \"number of estimators\" to be 103. Once we complete training the model, we get an accuracy measure of 81.48%\n",
    "\n",
    "_Gradient Boosting Classifier_\n",
    "\n",
    "One last model to consider is a Gradient Boosting Classifier. Using the same training/testing split as before and using Grid Search, we conclude the optimal hyper parameters are a learning rate of \"1\" and \"n_estimators\" of 54. When we train the model and then test its accuracy, we get an accuracy of 79.63% which is quite low and therefore we won't further consider the Gradient Boosting Classifier.\n",
    "\n",
    "_Precision Consideration_\n",
    "\n",
    "In a situation like this, we need to consider the gravity of False Negatives vs. False Positives. In general, it's probably far worse to have False Positive (predicting someone will get employement when in fact they will not get employment) as opposed to a False Negative (predicting someone will be unemployment but does end up getting a job). As such, the precision of a model will be something to consider as this is an indication of how often a model gets False Positives. For our Random Forest Classifier, find it has a precision score of 82.93% while our Logistic Regression Model has a precision score of 86.05%.\n",
    "\n",
    "Due to the Logistic Regression Model having a higher accuracy score and higher precision score, we conclude that it is the best model for this problem and therefore recommend the Logisitic Regression Model for implementation.\n",
    "\n",
    "_Logistic Model Evaluation_\n",
    "\n",
    "To gain some insights about our model, we can look a few aspects of it. The first two aspects we'll consider are the confusion matrix and the ROC curve of the Logistic Regression Model\n",
    "\n",
    "![alt text](ROC_Confusion_Log_COMPLETE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f829a63",
   "metadata": {},
   "source": [
    "The ROC curve gives us confidence that our model is performing fairly well. One consideration it does show us is that we could chance our threshold to label an onservation \"Positive\" (meaning predicting employement) to increase our rate of getting True Positives with the tradeoff of getting more False Positives. However, given the context of this problem where a False Positive has more costly connotations than a False Negative, we'll keep our model as is. If we check the Confusion Matrix, we can see that our model avoided all False Negatives but did end up with *some* False Positives. This may mean we can consider whether we are better off having some predictions as False Negatives if it means we can decrease the number of False Positives. Otherwise, the model performed quite well at accurately predicting student employment outcomes. One last thing we can check is the magnitudes of our coefficients to get a sense of how pertient any given input feature was in predicting employment.\n",
    "\n",
    "![alt text](Logit_CoeffMags_COMPLETE.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f5886b",
   "metadata": {},
   "source": [
    "When looking at the magnitudes, a few things stand out as either observations or things that could potentially warrant further evaluation:\n",
    "\n",
    "* Being female *decreases* the probability someone will be predicted as employed which may be cause for concern at the organization. It may need to consider whether females are being given the support they need or if there are systemic factors disadvantaging females that need to rectified.\n",
    "* Someone's \"ssc_p\", \"hsc_p\", \"degree_p\" (different testing results), and having \"work_experience\" appear to have the highest effect on positively predicting someone will have employment. If these are features that are determine *before* a student enters the institution, it may call into question how effective the institution is at providing value to students in terms of gaining employment. The institution may need to consider how it can support students who come in with \"pre-determined\" features to get them up to speed. As a recommendation, the institution would want to see pre-determined features *decrease* in terms of magnitude while seeing features determined while at the institution *increase*.\n",
    "* Oddly enough, \"mba_p\", has a *negative* magnitude with gaining employement which seems counter-intuitive if the organization is based on business education. That said, it may warrant further investigation regarding the value of getting high marks if employers don't seem to be particularly concerned about it. This may also suggest that activities that take away from studying *but* can contribute to gaining employment (e.g., time spent on informational interviews, networking, internships, etc.) may provide more value in getting employment. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24823103",
   "metadata": {},
   "source": [
    "#### _Modeling Salary_\n",
    "\n",
    "**It's important to note that this charge from the sponser is simply to do preliminary research to test model accuracy per the scope of our contract. If further work on predicting salary is desired, a new contract for this project will need to be drafted.**\n",
    "\n",
    "_Linear Regression_\n",
    "\n",
    "Since \"salary\" is a continuous variable, a good starting point is linear regression. For this portion, we conduct a new train/test split with 25% of the data being in the testing set. For this Linear Regression, we'll use Lasso regression. With Kfold regression. As a result, we get an accuracy score (or R<sup>2</sup>) of 81.56% which would give us some confidence that predicting salary is at least feasible\n",
    "\n",
    "_Random Forest Regressor_\n",
    "\n",
    "Another regression model we can use is the Random Forest Regressor model. Using the same train/test split and Grid Search, we find the best hyper parameters to consider are a \"max depth\" of 2 and \"n_estimators\" of 134. Once we test the model, we end up with an accuracy score of 80.42%\n",
    "\n",
    "_KNN Regressor Model_\n",
    "\n",
    "A final regression test we conduct is a KNN Regressor Model. Using the same train/test split and Grid Search, we find the best hyper parameters are \"n_neighbors\" at 7 and best weighting scheme to be \"distance\". However, our accuracy score for this model comes out to be -7.28% which would imply that either this model is not well-suited for our data *or* that our data needs further processing to be better evaluated by a model like this.\n",
    "\n",
    "_Salary Regression Conclusion_\n",
    "\n",
    "Based on the accuracy scores of the Lasso Linear Regression and Random Forest Regressor, it would seem possible to develop a model to make predictions on salary and for them to be fairly accurate *but* it appears that some more processing (and potentially feature engineering) would be necessary to get accuracy scores high enough to consider the models \"satisfactory.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70770cb",
   "metadata": {},
   "source": [
    "### 7) Conclusion\n",
    "\n",
    "We have successfully developed a Logistic Regression model for predicting Employment for our sponsor. In addition, the model identified some key considerations the organization may need to consider when meeting the needs of their students regarding gain employment!\n",
    "\n",
    "As a personal note, this was a really fun Capstone project for me and I definitely learned a lot! I'm looking forward to further improving my Data Science Skills! \n",
    "\n",
    "Cheers!\n",
    "Emre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06474986",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
